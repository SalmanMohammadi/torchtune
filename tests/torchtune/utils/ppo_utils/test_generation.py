# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

import pytest

import torch
from tests.test_utils import fixed_init_model
from torchtune.models.llama2 import llama2
from torchtune.utils import ppo_utils


class TestGenerate:
    """
    Test class for text generation functionality in :func:`~torchtune.utils.ppo_utils.generate`.
    See `torchtune.tests.utils.test_generation` for context.
    """

    @pytest.fixture
    def generation_model(self):
        model = llama2(
            vocab_size=4_000,
            embed_dim=128,
            num_layers=2,
            num_heads=4,
            num_kv_heads=4,
            max_seq_len=2048,
        )
        fixed_init_model(model)
        model.eval()
        return model

    @pytest.fixture
    def prompt_tokens(self):
        """
        Pytest fixture to create a list of prompt tokens for testing.
        """
        return torch.arange(2, 10)

    @pytest.fixture
    def padded_prompt_tokens(self):
        """
        Pytest fixture to create a list of left-padded prompt tokens for testing.
        """
        return torch.cat([torch.tensor([0, 0]), torch.arange(2, 10)])

    @pytest.fixture
    def prompt_tokens_batched_padded(self):
        """
        Pytest fixture to create a list of left-padded batched prompt tokens for testing.
        """
        return torch.cat([torch.tensor([0, 0]), torch.arange(2, 10)]).repeat(3, 1)

    @pytest.fixture
    def prompt_tokens_batched(self):
        return torch.arange(2, 10).repeat(3, 1)

    def test_reproducability_with_and_without_padding_batched(
        self,
        generation_model,
        prompt_tokens_batched_padded,
        prompt_tokens_batched,
    ):
        """
        Test to check if the `generate` function produces the same output for inputs that are left padded
        and for the same inputs that are not left padded, for a batch of inputs with varying sequence lengths.
        """
        temperature = 0.6
        top_k = 100

        torch.manual_seed(42)
        outputs = ppo_utils.generate(
            model=generation_model,
            prompt=prompt_tokens_batched_padded,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
        )

        torch.manual_seed(42)
        expected_outputs = ppo_utils.generate(
            model=generation_model,
            prompt=prompt_tokens_batched,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
        )

        assert [output[2:] for output in outputs] == expected_outputs

    def test_reproducability_with_and_without_padding(
        self, generation_model, prompt_tokens, padded_prompt_tokens
    ):
        """
        Test to check if the `generate` function produces the same output for inputs that are left padded
        and for the same inputs that are not left padded.
        """
        temperature = 0.6
        top_k = 100

        torch.manual_seed(42)
        outputs_unpadded = ppo_utils.generate(
            model=generation_model,
            prompt=prompt_tokens,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
        )

        torch.manual_seed(42)
        outputs_padded = ppo_utils.generate(
            model=generation_model,
            prompt=padded_prompt_tokens,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
        )

        assert outputs_unpadded[0] == outputs_padded[0][2:]

    def test_stop_tokens(self, generation_model, prompt_tokens):
        """
        Test to check if the `generate` function produces the right output when stop tokens are
        provided.
        """
        temperature = 0.6
        top_k = 100

        # This is the first token generated by the model
        # so it should stop immediately
        stop_tokens = [3983]

        torch.manual_seed(42)
        outputs = ppo_utils.generate(
            model=generation_model,
            prompt=prompt_tokens,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
            stop_tokens=stop_tokens,
        )

        expected_output = [[2, 3, 4, 5, 6, 7, 8, 9, 3983]]
        assert outputs == expected_output

    def test_stop_tokens_batched(self, generation_model, prompt_tokens_batched):
        """
        Test to check if the `generate` function produces the right output when stop tokens are
        provided, but this time in batched format.
        """
        temperature = 0.6
        top_k = 100

        # These are the first tokens generated by the model
        # so it should stop immediately
        stop_tokens = [3983, 3953, 3989]

        torch.manual_seed(42)

        outputs = ppo_utils.generate(
            model=generation_model,
            prompt=prompt_tokens_batched,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
            stop_tokens=stop_tokens,
        )

        expected_outputs = [
            [2, 3, 4, 5, 6, 7, 8, 9, 3983],
            [2, 3, 4, 5, 6, 7, 8, 9, 3953],
            [2, 3, 4, 5, 6, 7, 8, 9, 3989],
        ]

        assert outputs == expected_outputs

    def test_stop_tokens_batched_uneven(self, generation_model, prompt_tokens_batched):
        """
        Test to check if the `generate` function produces the right output when stop tokens are
        provided, but this time in batched format with different stopping lengths.
        """
        temperature = 0.6
        top_k = 100

        stop_tokens = [3962, 3953, 3999]

        torch.manual_seed(42)

        outputs = ppo_utils.generate(
            model=generation_model,
            prompt=prompt_tokens_batched,
            max_generated_tokens=10,
            temperature=temperature,
            top_k=top_k,
            stop_tokens=stop_tokens,
        )

        expected_outputs = [
            [2, 3, 4, 5, 6, 7, 8, 9, 3983, 3950, 3962],
            [2, 3, 4, 5, 6, 7, 8, 9, 3953, 0, 0],
            [2, 3, 4, 5, 6, 7, 8, 9, 3989, 3999, 0],
        ]

        assert outputs == expected_outputs


class TestGetCausalMask:
    @pytest.fixture
    def padded_prompt_tokens(self):
        """
        Pytest fixture to create a list of left-padded prompt tokens for testing.
        """
        return torch.cat([torch.tensor([0, 0]), torch.arange(2, 6)]).unsqueeze(0)

    @pytest.fixture
    def padded_prompt_tokens_batched(self):
        """
        Pytest fixture to create a list of left-padded batched prompt tokens for testing.
        """
        return torch.tensor(
            [[0, 0, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5], [0, 0, 0, 0, 0, 1]]
        )

    def test_causal_mask(self, padded_prompt_tokens):
        """
        Test to check if the `get_causal_mask` function produces the right output when a prompt is provided.
        """
        fill_value = torch.finfo(torch.float32).min
        attn_mask = torch.tensor(
            [
                [False, False, False, False, False, False],
                [False, False, False, False, False, False],
                [False, False, True, False, False, False],
                [False, False, True, True, False, False],
                [False, False, True, True, True, False],
                [False, False, True, True, True, True],
            ]
        )

        # We need to invert the mask to get the expected output
        expected_casual_mask = torch.zeros_like(
            attn_mask, dtype=torch.float32
        ).masked_fill(~attn_mask, fill_value)[None, :]
        causal_mask = ppo_utils.get_causal_mask(padded_prompt_tokens)
        torch.testing.assert_close(causal_mask, expected_casual_mask, atol=0, rtol=0)

    def test_causal_mask_batched(self, padded_prompt_tokens_batched):
        """
        Test to check if the `get_causal_mask` function produces the right output when a batched prompt is provided.
        """
        fill_value = torch.finfo(torch.float32).min
        attn_mask = torch.tensor(
            [
                [
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, False],
                    [False, False, False, True, False, False],
                    [False, False, False, True, True, False],
                    [False, False, False, True, True, True],
                ],
                [
                    [False, False, False, False, False, False],
                    [False, True, False, False, False, False],
                    [False, True, True, False, False, False],
                    [False, True, True, True, False, False],
                    [False, True, True, True, True, False],
                    [False, True, True, True, True, True],
                ],
                [
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, False],
                    [False, False, False, False, False, True],
                ],
            ]
        )

        # We need to invert the mask to get the expected output
        expected_casual_mask = torch.zeros_like(
            attn_mask, dtype=torch.float32
        ).masked_fill(~attn_mask, fill_value)
        causal_mask = ppo_utils.get_causal_mask(padded_prompt_tokens_batched)
        torch.testing.assert_close(causal_mask, expected_casual_mask, atol=0, rtol=0)
